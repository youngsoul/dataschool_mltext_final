{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSchool - Machine Learning with Text and Python:  \n",
    "\n",
    "### Week 6 Self Imposed Homework\n",
    "\n",
    "I am going to use the Kaggle competition that inspired me to sign up for this course.  Using what I have learned in the Machine Learning with Text course, I will attempt to beat the scores on the leader board.\n",
    "\n",
    "The Kaggle competition is:\n",
    "\n",
    "![SA Image](sa_emotions_picture.png) \n",
    "\n",
    "### [Sentiment Analysis: Emotion in Text.](https://www.kaggle.com/c/sa-emotions)\n",
    "\n",
    "*Identify emotion in text using sentiment analysis.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/youngsoul/Documents/Development/PythonDev/VirtualEnvs/MLText2Env/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from stemming.porter2 import stem\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sentiment_analysis.transformers import RemoveEllipseTransformer, RemoveHtmlEncodedTransformer, RemoveNumbersTransformer, RemoveSpecialCharactersTransformer, RemoveUsernameTransformer, RemoveUrlsTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, FunctionTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.base import TransformerMixin\n",
    "import re\n",
    "import nltk.stem\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from nltk.sentiment.util import mark_negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# allow plots to appear in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the training data\n",
    "training_data = pd.read_csv('../data/kaggle/sa-emotions/train_data_lexicon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>disposition</th>\n",
       "      <th>neg_words</th>\n",
       "      <th>neu_words</th>\n",
       "      <th>pos_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>worry</td>\n",
       "      <td>Re-pinging @ghostridah14: why didn't you go to...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>sadness</td>\n",
       "      <td>I should be sleep, but im not! thinking about ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>worry</td>\n",
       "      <td>Hmmm. http://www.djhero.com/ is down</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>sadness</td>\n",
       "      <td>@charviray Charlene my love. I miss you</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>sadness</td>\n",
       "      <td>@kelcouch I'm sorry  at least it's Friday?</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>neutral</td>\n",
       "      <td>cant fall asleep</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>worry</td>\n",
       "      <td>Choked on her retainers</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Ugh! I have to beat this stupid song to get to...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>sadness</td>\n",
       "      <td>@BrodyJenner if u watch the hills in london u ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Got the news</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>sadness</td>\n",
       "      <td>The storm is here and the electricity is gone</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>love</td>\n",
       "      <td>@annarosekerr agreed</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>sadness</td>\n",
       "      <td>So sleepy again and it's not even that late. I...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>worry</td>\n",
       "      <td>@PerezHilton lady gaga tweeted about not being...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>sadness</td>\n",
       "      <td>How are YOU convinced that I have always wante...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0   sentiment                                            content  \\\n",
       "0            0       empty  @tiffanylue i know  i was listenin to bad habi...   \n",
       "1            1     sadness  Layin n bed with a headache  ughhhh...waitin o...   \n",
       "2            2     sadness                Funeral ceremony...gloomy friday...   \n",
       "3            3  enthusiasm               wants to hang out with friends SOON!   \n",
       "4            4     neutral  @dannycastillo We want to trade with someone w...   \n",
       "5            5       worry  Re-pinging @ghostridah14: why didn't you go to...   \n",
       "6            6     sadness  I should be sleep, but im not! thinking about ...   \n",
       "7            7       worry               Hmmm. http://www.djhero.com/ is down   \n",
       "8            8     sadness            @charviray Charlene my love. I miss you   \n",
       "9            9     sadness         @kelcouch I'm sorry  at least it's Friday?   \n",
       "10          10     neutral                                   cant fall asleep   \n",
       "11          11       worry                            Choked on her retainers   \n",
       "12          12     sadness  Ugh! I have to beat this stupid song to get to...   \n",
       "13          13     sadness  @BrodyJenner if u watch the hills in london u ...   \n",
       "14          14    surprise                                       Got the news   \n",
       "15          15     sadness      The storm is here and the electricity is gone   \n",
       "16          16        love                               @annarosekerr agreed   \n",
       "17          17     sadness  So sleepy again and it's not even that late. I...   \n",
       "18          18       worry  @PerezHilton lady gaga tweeted about not being...   \n",
       "19          19     sadness  How are YOU convinced that I have always wante...   \n",
       "\n",
       "    disposition  neg_words  neu_words  pos_words  \n",
       "0          -1.0   0.050000   0.950000   0.000000  \n",
       "1          -1.0   0.076923   0.923077   0.000000  \n",
       "2          -1.0   0.166667   0.833333   0.000000  \n",
       "3          -1.0   0.125000   0.875000   0.000000  \n",
       "4           0.0   0.000000   1.000000   0.000000  \n",
       "5           1.0   0.000000   0.950000   0.050000  \n",
       "6          -1.0   0.058824   0.941176   0.000000  \n",
       "7           0.0   0.000000   1.000000   0.000000  \n",
       "8          -1.0   0.125000   0.875000   0.000000  \n",
       "9          -1.0   0.090909   0.909091   0.000000  \n",
       "10         -1.0   0.333333   0.666667   0.000000  \n",
       "11          0.0   0.000000   1.000000   0.000000  \n",
       "12         -1.0   0.187500   0.812500   0.000000  \n",
       "13          0.0   0.000000   1.000000   0.000000  \n",
       "14          0.0   0.000000   1.000000   0.000000  \n",
       "15          0.0   0.000000   1.000000   0.000000  \n",
       "16          0.0   0.000000   1.000000   0.000000  \n",
       "17         -1.0   0.066667   0.933333   0.000000  \n",
       "18          0.0   0.058824   0.882353   0.058824  \n",
       "19         -1.0   0.076923   0.923077   0.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RemoveUsernameTransformer(TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer that will remove tokens from a string of the form:  @someusername\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _preprocess_data(data_series):\n",
    "        \"\"\"\n",
    "        inspired from:\n",
    "        https://raw.githubusercontent.com/youngsoul/ml-twitter-sentiment-analysis/develop/cleanup.py\n",
    "        :param data_series:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # remove user name\n",
    "        regex = re.compile(r\"@[^\\s]+[\\s]?\")\n",
    "        data_series.replace(regex, \"\", inplace=True)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "\n",
    "        :param X: Series, aka column of data.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        RemoveUsernameTransformer._preprocess_data(X)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RemoveNumbersTransformer(TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer that will remove tokens from a string that are numbers.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _preprocess_data(data_series):\n",
    "        \"\"\"\n",
    "        inspired from:\n",
    "        https://raw.githubusercontent.com/youngsoul/ml-twitter-sentiment-analysis/develop/cleanup.py\n",
    "        :param data_series:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # remove numbers\n",
    "        regex = re.compile(r\"\\s?[0-9]+\\.?[0-9]*\")\n",
    "        data_series.replace(regex, \"\", inplace=True)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "\n",
    "        :param X: Series, aka column of data.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        RemoveNumbersTransformer._preprocess_data(X)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    \"\"\"\n",
    "    A TF-IDF Vectorizer that will apply a stemmer to the tokeninze word.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, stemmer=None, input='content', encoding='utf-8',\n",
    "                 decode_error='strict', strip_accents=None, lowercase=True,\n",
    "                 preprocessor=None, tokenizer=None, analyzer='word',\n",
    "                 stop_words=None, token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "                 ngram_range=(1, 1), max_df=1.0, min_df=1,\n",
    "                 max_features=None, vocabulary=None, binary=False,\n",
    "                 dtype=np.int64, norm='l2', use_idf=True, smooth_idf=True,\n",
    "                 sublinear_tf=False):\n",
    "\n",
    "        super(StemmedTfidfVectorizer, self).__init__(\n",
    "            input=input, encoding=encoding, decode_error=decode_error,\n",
    "            strip_accents=strip_accents, lowercase=lowercase,\n",
    "            preprocessor=preprocessor, tokenizer=tokenizer, analyzer=analyzer,\n",
    "            stop_words=stop_words, token_pattern=token_pattern,\n",
    "            ngram_range=ngram_range, max_df=max_df, min_df=min_df,\n",
    "            max_features=max_features, vocabulary=vocabulary, binary=binary,\n",
    "            dtype=dtype, norm='l2', use_idf=True, smooth_idf=True,\n",
    "                             sublinear_tf=False)\n",
    "        self.stemmer = stemmer\n",
    "\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([self.stemmer.stem(w) for w in analyzer(doc)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mark_negation_sentence(sentence):\n",
    "    \"\"\"\n",
    "    See the NLTK utility for the mark_negation function.\n",
    "    \n",
    "    This function will take a sentence in, split it and call mark_negation, and \n",
    "    puts the string back together again.  \n",
    "    \n",
    "    Append _NEG suffix to words that appear in the scope between a negation\n",
    "    and a punctuation mark.\n",
    "    \n",
    "    :param sentence an entire sentence\n",
    "    :return sentence with the negation marked\n",
    "    \"\"\"\n",
    "    return \" \".join(mark_negation(sentence.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions used to create different features from the text.\n",
    "def count_username_mentions(value):\n",
    "    return len(re.findall(r\"@[^\\s]+[\\s]?\", value))\n",
    "\n",
    "def count_ellipsis(value):\n",
    "    return len(re.findall(r\"\\.\\s?\\.\\s?\\.\", value))\n",
    "\n",
    "def count_hashtags(value):\n",
    "    return len(re.findall(r\"3[^\\s]+[\\s]?\", value))\n",
    "\n",
    "def count_exclamation_points(value):\n",
    "    groups = re.findall(r\"\\w+(!+)\", value)\n",
    "    return sum([len(exclamation_string) for exclamation_string in groups])\n",
    "\n",
    "def count_question_marks(value):\n",
    "    groups = re.findall(r\"[\\w+!](\\?+)\", value)\n",
    "    return sum([len(exclamation_string) for exclamation_string in groups])\n",
    "\n",
    "def is_boredom(y):\n",
    "    if 'bored' in y.lower() or 'boring' in y.lower():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_features(df):\n",
    "    df['number_of_mentions'] = df.content.apply(count_username_mentions)\n",
    "    df['number_of_ellipsis'] = df.content.apply(count_ellipsis)\n",
    "    df['number_of_exclamations'] = df.content.apply(count_exclamation_points)\n",
    "    df['number_of_hashtabs'] = df.content.apply(count_hashtags)\n",
    "    df['number_of_question'] = df.content.apply(count_question_marks)\n",
    "    df['is_boredom'] = df.content.apply(is_boredom)\n",
    "    #df['content_len'] = df.content.apply(len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read the data, setup the function transformers, add the features to the training data.\n",
    "\"\"\"\n",
    "\n",
    "# -----------------  Function Transformers ----------------\n",
    "def get_features_df(df):\n",
    "    return df.loc[:, ['number_of_mentions', 'number_of_ellipsis', 'number_of_exclamations', 'number_of_hashtabs', 'number_of_question', 'is_boredom', 'disposition', 'neg_words', 'neu_words', 'pos_words']]\n",
    "\n",
    "def get_sentiment_content(df):\n",
    "    '''Returns the original content from the data set'''\n",
    "    return df.content.copy()\n",
    "\n",
    "def get_sentiment_content_negation(df):\n",
    "    '''Returns the content after it has gone through negation'''\n",
    "    return df.content_negation.copy()\n",
    "\n",
    "def get_sentiment_content_preprocess_negation(df):\n",
    "    '''Returns the content after is has gone through negation AND preprocessed'''\n",
    "    return df.content_preprocessed_negation\n",
    "\n",
    "\n",
    "# create a function transformer to just extract the feature columns\n",
    "get_features_transformer = FunctionTransformer(get_features_df, validate=False)\n",
    "# usage: get_features_transformer.transform(training_data_with_features).head()\n",
    "\n",
    "# create a function transformer to return the sentiment content so it can be used in pipeline/union\n",
    "get_sentiment_content_transformer = FunctionTransformer(get_sentiment_content, validate=False)\n",
    "\n",
    "get_sentiment_content_negation_transformer = FunctionTransformer(get_sentiment_content_negation, validate=False)\n",
    "\n",
    "get_sentiment_content_preprocess_negation_transformer = FunctionTransformer(get_sentiment_content_preprocess_negation, validate=False)\n",
    "\n",
    "# -----------------  End Function Transformers ----------------\n",
    "\n",
    "def preprocess_data_set(input_data_set):\n",
    "    # Create a pipeline with the transformers we are keeping, and see the overall improvement.\n",
    "    # This pipeline gets a little tricky.  the 'get_sentiment_content_transformer' returns a COPY of the\n",
    "    # original content.  So the transformers work on a copy of the content, leaving the make_features with the\n",
    "    # original content to create features from.\n",
    "    preprocessor_pipeline = make_pipeline(get_sentiment_content_transformer, RemoveNumbersTransformer(), RemoveUsernameTransformer())\n",
    "    preprocessed_training_data_content = preprocessor_pipeline.transform(input_data_set)\n",
    "    input_data_set['content_preprocessed'] = preprocessed_training_data_content\n",
    "    input_data_set['content_preprocessed_negation'] = input_data_set.content_preprocessed.apply(mark_negation_sentence)\n",
    "\n",
    "    make_features(input_data_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocess_data_set(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 15)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33674193257108043"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# create stemmer and vectorizer\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "stemmed_tfidf_vectorizer = StemmedTfidfVectorizer(stemmer=stemmer, min_df=5, max_df=0.8, ngram_range=(1,4), stop_words='english', sublinear_tf=True)\n",
    "\n",
    "# UNION\n",
    "#    PIPE\n",
    "#        get the negated and preprocessed text\n",
    "#        send to stemmed tfidf vectorizer to create vocabulary and document term matrix\n",
    "#    Get the features DataFrame transformer\n",
    "union = make_union(make_pipeline(get_sentiment_content_preprocess_negation_transformer, stemmed_tfidf_vectorizer),\n",
    "                  get_features_transformer)\n",
    "\n",
    "# encode the sentiment outcomes as a number using the LabelEncoder\n",
    "# would like to create a column, e.g. sentiment_num, which is a numeric representation of the sentiment.\n",
    "# this will have to also be applied to any test data.\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# fit the label encoder with the unique set of sentiments in the training data.\n",
    "label_encoder.fit(training_data.sentiment.unique())\n",
    "\n",
    "# Create an outcomes which is the numeric representation of the label\n",
    "y = training_data.sentiment.apply(lambda x: label_encoder.transform([x])[0])\n",
    "\n",
    "# create LogisticRegression Model\n",
    "# 0.3250\n",
    "model = LogisticRegression(C=0.1)\n",
    "\n",
    "# 0.307\n",
    "#model = MultinomialNB()\n",
    "\n",
    "# Model Pipeline\n",
    "#    UNION ->Model\n",
    "# The union will create a traditional vectoizered set of tokens, and a non-DTM data frame for \n",
    "# the model.\n",
    "model_pipeline = make_pipeline(union, model)\n",
    "cross_val_score(model_pipeline, training_data, y, cv=5, scoring='accuracy').mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the pos/neg/neu/disposition columns, the new accuracy score is now 0.3367"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the test data that will be used to submit to Kaggle\n",
    "test_data = pd.read_csv('../data/kaggle/sa-emotions/test_data_lexicon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>disposition</th>\n",
       "      <th>neg_words</th>\n",
       "      <th>neu_words</th>\n",
       "      <th>pos_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>is hangin with the love of my life. Tessa McCr...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>I've Got An Urge To Make Music Like Massively....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>@lacrossehawty rofl uh huh</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>@fankri haha! thanks, Tiff   it went well, but...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>@alyssaisntcool hahah  i loveeee him though.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id                                            content  \\\n",
       "0           0   1  is hangin with the love of my life. Tessa McCr...   \n",
       "1           1   2  I've Got An Urge To Make Music Like Massively....   \n",
       "2           2   3                         @lacrossehawty rofl uh huh   \n",
       "3           3   4  @fankri haha! thanks, Tiff   it went well, but...   \n",
       "4           4   5       @alyssaisntcool hahah  i loveeee him though.   \n",
       "\n",
       "   disposition  neg_words  neu_words  pos_words  \n",
       "0          1.0        0.0   0.916667   0.083333  \n",
       "1          1.0        0.0   0.937500   0.062500  \n",
       "2          0.0        0.0   1.000000   0.000000  \n",
       "3          1.0        0.0   0.962963   0.037037  \n",
       "4          0.0        0.0   1.000000   0.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>disposition</th>\n",
       "      <th>neg_words</th>\n",
       "      <th>neu_words</th>\n",
       "      <th>pos_words</th>\n",
       "      <th>content_preprocessed</th>\n",
       "      <th>content_preprocessed_negation</th>\n",
       "      <th>number_of_mentions</th>\n",
       "      <th>number_of_ellipsis</th>\n",
       "      <th>number_of_exclamations</th>\n",
       "      <th>number_of_hashtabs</th>\n",
       "      <th>number_of_question</th>\n",
       "      <th>is_boredom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>is hangin with the love of my life. Tessa McCr...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>is hangin with the love of my life. Tessa McCr...</td>\n",
       "      <td>is hangin with the love of my life. Tessa McCr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>I've Got An Urge To Make Music Like Massively....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>I've Got An Urge To Make Music Like Massively....</td>\n",
       "      <td>I've Got An Urge To Make Music Like Massively....</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>@lacrossehawty rofl uh huh</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>rofl uh huh</td>\n",
       "      <td>rofl uh huh</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>@fankri haha! thanks, Tiff   it went well, but...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>haha! thanks, Tiff   it went well, but they WO...</td>\n",
       "      <td>haha! thanks, Tiff it went well, but they WORE...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>@alyssaisntcool hahah  i loveeee him though.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>hahah  i loveeee him though.</td>\n",
       "      <td>hahah i loveeee him though.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id                                            content  \\\n",
       "0           0   1  is hangin with the love of my life. Tessa McCr...   \n",
       "1           1   2  I've Got An Urge To Make Music Like Massively....   \n",
       "2           2   3                         @lacrossehawty rofl uh huh   \n",
       "3           3   4  @fankri haha! thanks, Tiff   it went well, but...   \n",
       "4           4   5       @alyssaisntcool hahah  i loveeee him though.   \n",
       "\n",
       "   disposition  neg_words  neu_words  pos_words  \\\n",
       "0          1.0        0.0   0.916667   0.083333   \n",
       "1          1.0        0.0   0.937500   0.062500   \n",
       "2          0.0        0.0   1.000000   0.000000   \n",
       "3          1.0        0.0   0.962963   0.037037   \n",
       "4          0.0        0.0   1.000000   0.000000   \n",
       "\n",
       "                                content_preprocessed  \\\n",
       "0  is hangin with the love of my life. Tessa McCr...   \n",
       "1  I've Got An Urge To Make Music Like Massively....   \n",
       "2                                        rofl uh huh   \n",
       "3  haha! thanks, Tiff   it went well, but they WO...   \n",
       "4                       hahah  i loveeee him though.   \n",
       "\n",
       "                       content_preprocessed_negation  number_of_mentions  \\\n",
       "0  is hangin with the love of my life. Tessa McCr...                   0   \n",
       "1  I've Got An Urge To Make Music Like Massively....                   0   \n",
       "2                                        rofl uh huh                   1   \n",
       "3  haha! thanks, Tiff it went well, but they WORE...                   1   \n",
       "4                        hahah i loveeee him though.                   1   \n",
       "\n",
       "   number_of_ellipsis  number_of_exclamations  number_of_hashtabs  \\\n",
       "0                   0                       2                   0   \n",
       "1                   0                       0                   0   \n",
       "2                   0                       0                   0   \n",
       "3                   0                       4                   0   \n",
       "4                   0                       0                   0   \n",
       "\n",
       "   number_of_question  is_boredom  \n",
       "0                   0           0  \n",
       "1                   0           0  \n",
       "2                   0           0  \n",
       "3                   0           0  \n",
       "4                   0           0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre-process the test data, because pre-processing should happen on the training and the testing data.\n",
    "preprocess_data_set(test_data)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 15)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the model pipeline to make predictions\n",
    "\n",
    "Once we have the test data preprocessed - we use the model pipeline with all of the training data, and predict on the testing data.\n",
    "\n",
    "For a pipeline, we can treat it just like a regular model and call 'fit' and 'predict'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('featureunion', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('pipeline', Pipeline(memory=None,\n",
       "     steps=[('functiontransformer', FunctionTransformer(accept_sparse=False,\n",
       "          func=<function get_sentiment_content_preprocess_negation at 0x10d711620>,\n",
       "          inv_kw_args=None, inve...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipeline.fit(training_data, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_class = model_pipeline.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love' 'neutral' 'neutral' ..., 'happiness' 'happiness' 'neutral']\n"
     ]
    }
   ],
   "source": [
    "# convert the y_pred_class classification numbers BACK to their string versions for submission\n",
    "y_pred_class_labels = label_encoder.inverse_transform(y_pred_class)\n",
    "print(y_pred_class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a submission file (resulting score: 0.30040)\n",
    "# sub1 \n",
    "# sub2 - added remove html characters\n",
    "# sub3 = sub1, sanity check\n",
    "# sub4 = added pos/neg/neu/disposition to the data set.  resulting score: 0.33700\n",
    "pd.DataFrame({'id':test_data.id, 'sentiment':y_pred_class_labels}).set_index('id').to_csv('../data/kaggle/sa-emotions/sub4.csv')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
